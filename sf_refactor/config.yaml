defaults:
  - _self_
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled
  
# Prevent hydra from automatically creating output directories
hydra:
  output_subdir: null
  run:
    dir: .

# Model parameters
model:
  dim_cutoff: 6 
  wires: 3
  # layers: 1 # not implemented yet
  photon_modes: ${model.wires}  # Default to same as wires, can be overridden
  which_circuit: "new"  # "default", "new", "multiuploading", "reuploading", "maximally_entangled", or "new_entangled"
  particles_per_wire: 2  # Only used with "multiuploading" circuit - number of particles to encode per wire
                         # Note: wires Ã— particles_per_wire must not exceed available particles in data
  particle_mapping: "interleaved"  # "sequential" or "interleaved" - how to map particles to wires
                                   # sequential: wire0:[0,1], wire1:[2,3], wire2:[4,5] etc.
                                   # interleaved: wire0:[0,4], wire1:[1,5], wire2:[2,6] etc. (for 4 wires)
  reuploads_per_wire: 2  # Only used with "reuploading" circuit - times to re-encode the same particle per wire
                                   
                                   

# Training parameters
training:
  epochs: 20
  learning_rate: 0.005
  batch_size: 8
  loss_fn: "bce"  # "bce" or "mse"
  tanh: false

# Data parameters
data:
  train_jets: 100 # /40000
  val_jets: 20 # /10000
  test_jets: 120 # /20000
  data_dir: "/rds/general/user/lr1424/home/1P1Qm_SF/flat_train/TTBar+ZJets_flat.h5"
  val_dir: "/rds/general/user/lr1424/home/1P1Qm_SF/flat_val/TTBar+ZJets_flat.h5"
  test_dir: "/rds/general/user/lr1424/home/1P1Qm_SF/flat_test/TTBar+ZJets_flat.h5"
  # Deprecated: unified save_dir kept for backward compatibility in readers.
  save_dir: "/rds/general/user/lr1424/home/1P1Qm_SF/sf_refactor/saved_models_sf"
  # New: separate base directories for classifier and autoencoder outputs
  classifier_save_dir: "/rds/general/user/lr1424/home/1P1Qm_SF/sf_refactor/saved_models_sf/classifier"
  autoencoder_save_dir: "/rds/general/user/lr1424/home/1P1Qm_SF/sf_refactor/saved_models_sf/autoencoder"

# Early stopping and learning rate parameters
optimization:
  patience: 4  # Number of epochs to wait for improvement before early stopping
  min_delta: 0.001  # Minimum change in validation AUC to qualify as improvement
  restore_best: true  # Restore weights from best validation epoch when early stopping
  lr_patience: 3  # Number of epochs to wait before reducing learning rate
  lr_factor: 0.5  # Factor to multiply learning rate by when reducing (new_lr = old_lr * factor)
  min_lr: 1e-5  # Minimum learning rate threshold (stops reducing below this)
  min_epochs: 10  # Minimum number of epochs to train before allowing early stopping

# Runtime parameters
runtime:
  cli_test: false
  run_name: null  # Will be set via command line or auto-generated

# Memory profiling parameters
profiling:
  memory_enabled: true
  memory_log_frequency: 5  # Log memory every N batches (0 = only epochs)

# Autoencoder settings
autoencoder:
  # Indexes of qumodes to be "trashed" (pushed to vacuum)
  trash_modes: [0, 1]
  # Label to train on exclusively (0=background, 1=signal). For anomaly detection, use 0.
  train_label: 0

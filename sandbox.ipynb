{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acab6cd",
   "metadata": {},
   "source": [
    "# Reading .h5 keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3f01c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['PFCand_subjet_idx', 'jetConstituentNames', 'jetConstituentsExtra', 'jetConstituentsExtraNames', 'jetConstituentsList', 'jetFeatureNames', 'jetFeatures', 'num_PFCands_subleading_jet', 'subjet_feature_names', 'subjet_features', 'subjet_labels', 'truth_labels']>\n",
      "Unique entries in 'jetConstituentNames': {'pt', 'phi', 'eta'}\n",
      "Unique entries in 'jetConstituentsExtraNames': {'part_isChargedHadron', 'part_dzval', 'part_d0err', 'part_isMuon', 'part_isElectron', 'part_charge', 'part_isPhoton', 'part_isNeutralHadron', 'part_dzerr', 'part_d0val'}\n",
      "Unique entries in 'jetFeatureNames': {'jet_phi', 'jet_tau2', 'jet_pt', 'jet_energy', 'jet_tau1', 'jet_nparticles', 'jet_eta', 'jet_tau4', 'jet_tau3', 'jet_sdmass'}\n",
      "Unique entries in 'subjet_feature_names': {'sj1Eta', 'sj2Pt', 'sj1Pt', 'sj2Eta', 'sj2Phi', 'sj1Phi', 'sj1E', 'sj2E'}\n",
      "Unique entries in 'subjet_labels': {0.0, 1.0, -1.0}\n",
      "Unique entries in 'truth_labels': {0.0, 1.0}\n",
      "Shape of 'jetConstituentsList': (40000, 100, 3)\n",
      "Shape of 'jetConstituentNames': (3,)\n",
      "['eta', 'phi', 'pt']\n",
      "Shape of 'truth_labels': (40000,)\n",
      "[0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Jet 0:\n",
      "  Particle 0: eta=-4.738°, phi=16.658°, pT=98.566\n",
      "  Particle 1: eta=-4.343°, phi=12.881°, pT=86.304\n",
      "  Particle 2: eta=2.133°, phi=-19.186°, pT=31.539\n",
      "  Particle 3: eta=-4.180°, phi=12.797°, pT=29.191\n",
      "  Particle 4: eta=-4.367°, phi=12.028°, pT=27.033\n",
      "\n",
      "Jet 1:\n",
      "  Particle 0: eta=0.368°, phi=6.930°, pT=78.996\n",
      "  Particle 1: eta=-13.221°, phi=-10.099°, pT=41.021\n",
      "  Particle 2: eta=0.352°, phi=7.606°, pT=37.576\n",
      "  Particle 3: eta=-1.354°, phi=8.993°, pT=32.619\n",
      "  Particle 4: eta=0.424°, phi=8.417°, pT=29.341\n",
      "\n",
      "Jet 2:\n",
      "  Particle 0: eta=-5.158°, phi=-6.119°, pT=96.594\n",
      "  Particle 1: eta=-5.600°, phi=-6.863°, pT=62.246\n",
      "  Particle 2: eta=-5.416°, phi=-5.320°, pT=51.986\n",
      "  Particle 3: eta=-4.651°, phi=-5.770°, pT=48.388\n",
      "  Particle 4: eta=-5.595°, phi=-5.775°, pT=43.907\n",
      "\n",
      "pt: min=0.000, max=979.512\n",
      "eta: min=-0.800, max=0.800\n",
      "phi: min=-0.797, max=0.798\n"
     ]
    }
   ],
   "source": [
    "import h5py,sys,os\n",
    "import numpy as np\n",
    "# Function to print the keys of an HDF5 file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tfilepath=\"/home/hep/lr1424/1P1Qm/flat_train/TTBar+ZJets_flat.h5\"\n",
    "\tf= h5py.File(filepath,'r')\n",
    "\tprint(f.keys())\n",
    "\n",
    "\n",
    "\t# Print unique entries for all non-data keys\n",
    "\tnon_data_keys = [\n",
    "\t\t'jetConstituentNames', 'jetConstituentsExtraNames',\n",
    "\t\t'jetFeatureNames', \n",
    "\t\t'subjet_feature_names', 'subjet_labels', 'truth_labels'\n",
    "\t]\n",
    "\n",
    "\tfor key in non_data_keys:\n",
    "\t\tdata = f[key][:]\n",
    "\t\tunique_entries = set(data.flat) if hasattr(data, 'flat') else set(data)\n",
    "\t\tprint(f\"Unique entries in '{key}':\", unique_entries)\n",
    "  \n",
    "# Get the shape of 'jetConstituentsList' dataset\n",
    "if 'jetConstituentsList' in f:\n",
    "\tshape = f['jetConstituentsList'].shape\n",
    "\tprint(f\"Shape of 'jetConstituentsList': {shape}\")\n",
    "else:\n",
    "\tprint(\"'jetConstituentsList' not found in file.\")\n",
    " \n",
    "shape1 = f['jetConstituentNames'].shape\n",
    "print(f\"Shape of 'jetConstituentNames': {shape1}\")\n",
    "print([x for x in f['jetConstituentNames']])\n",
    "\n",
    "shape2 = f['truth_labels'].shape\n",
    "print(f\"Shape of 'truth_labels': {shape2}\")\n",
    "print([x for x in f['truth_labels']][:10])  # Print first 10 truth labels\n",
    "# Print the first 3 jets and first 5 particles per jet with their pT, eta, phi\n",
    "jets = f['jetConstituentsList'][:3, :5, :3]  # shape: (3, 5, 3)\n",
    "for jet_idx, jet in enumerate(jets):\n",
    "\tprint(f\"Jet {jet_idx}:\")\n",
    "\tfor part_idx, (eta, phi, pt) in enumerate(jet):\n",
    "\t\teta_deg = np.degrees(eta)\n",
    "\t\tphi_deg = np.degrees(phi)\n",
    "\t\tprint(f\"  Particle {part_idx}: eta={eta_deg:.3f}°, phi={phi_deg:.3f}°, pT={pt:.3f}\")\n",
    "\tprint()\n",
    " \n",
    "# Find max and min pt, eta, and phi from the whole file\n",
    "jet_constituents = f['jetConstituentsList'][:]  # shape: (N_jets, N_particles, 3)\n",
    "eta_all = jet_constituents[..., 0]\n",
    "phi_all = jet_constituents[..., 1]\n",
    "pt_all = jet_constituents[..., 2]\n",
    "\n",
    "print(f\"pt: min={pt_all.min():.3f}, max={pt_all.max():.3f}\")\n",
    "print(f\"eta: min={eta_all.min():.3f}, max={eta_all.max():.3f}\")\n",
    "print(f\"phi: min={phi_all.min():.3f}, max={phi_all.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054c01c",
   "metadata": {},
   "source": [
    "# Observing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3c7313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/hep/lr1424/1P1Qm_fork/output_logs/2000_jets.log\n",
      "\t Epoch 1/10 - Training Loss: 0.6948 - Validation Loss: 0.6930\n",
      "\t Epoch 2/10 - Training Loss: 0.6948 - Validation Loss: 0.6930\n",
      "\n",
      "Processing /home/hep/lr1424/1P1Qm_fork/output_logs/100_jets_MSE.log\n",
      "\t Epoch 1/10 - Training Loss: 0.2514 - Validation Loss: 0.2501\n",
      "\t Epoch 2/10 - Training Loss: 0.2513 - Validation Loss: 0.2506\n",
      "\t Epoch 3/10 - Training Loss: 0.2513 - Validation Loss: 0.2509\n",
      "\t Epoch 4/10 - Training Loss: 0.2513 - Validation Loss: 0.2511\n",
      "\t Epoch 5/10 - Training Loss: 0.2513 - Validation Loss: 0.2512\n",
      "\t Epoch 6/10 - Training Loss: 0.2513 - Validation Loss: 0.2512\n",
      "\t Epoch 7/10 - Training Loss: 0.2513 - Validation Loss: 0.2513\n",
      "\t Epoch 8/10 - Training Loss: 0.2513 - Validation Loss: 0.2513\n",
      "\t Epoch 9/10 - Training Loss: 0.2513 - Validation Loss: 0.2513\n",
      "\t Epoch 10/10 - Training Loss: 0.2513 - Validation Loss: 0.2513\n",
      "\n",
      "Processing /home/hep/lr1424/1P1Qm_fork/output_logs/100_jets_BCE.log\n",
      "\t Epoch 1/10 - Training Loss: 0.6959 - Validation Loss: 0.6933\n",
      "\t Epoch 2/10 - Training Loss: 0.6957 - Validation Loss: 0.6943\n",
      "\t Epoch 3/10 - Training Loss: 0.6957 - Validation Loss: 0.6949\n",
      "\t Epoch 4/10 - Training Loss: 0.6957 - Validation Loss: 0.6953\n",
      "\t Epoch 5/10 - Training Loss: 0.6957 - Validation Loss: 0.6955\n",
      "\t Epoch 6/10 - Training Loss: 0.6957 - Validation Loss: 0.6956\n",
      "\t Epoch 7/10 - Training Loss: 0.6958 - Validation Loss: 0.6957\n",
      "\t Epoch 8/10 - Training Loss: 0.6958 - Validation Loss: 0.6958\n",
      "\t Epoch 9/10 - Training Loss: 0.6958 - Validation Loss: 0.6958\n",
      "\t Epoch 10/10 - Training Loss: 0.6958 - Validation Loss: 0.6958\n",
      "\n",
      "Processing /home/hep/lr1424/1P1Qm_fork/output_logs/1000_jets_MSE.log\n",
      "\t Epoch 1/10 - Training Loss: 0.2501 - Validation Loss: 0.2543\n",
      "\t Epoch 2/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\t Epoch 3/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\t Epoch 4/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\t Epoch 5/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\t Epoch 6/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\t Epoch 7/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\t Epoch 8/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\t Epoch 9/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\t Epoch 10/10 - Training Loss: 0.2505 - Validation Loss: 0.2543\n",
      "\n",
      "Processing /home/hep/lr1424/1P1Qm_fork/output_logs/sanity_check_1000_jets_MSE.log\n",
      "\n",
      "Processing /home/hep/lr1424/1P1Qm_fork/output_logs/sanity_check_2_1000_jets_MSE.log\n",
      "\n",
      "Processing /home/hep/lr1424/1P1Qm_fork/output_logs/sanity_check_2_1000_jets_BCE.log\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "log_files = glob.glob(\"/home/hep/lr1424/1P1Qm_fork/output_logs/*.log\")\n",
    "pattern = re.compile(r\"Epoch\\s+(\\d+)/(\\d+)\\s+-\\s+Training Loss:\\s+([\\d.]+)\\s+-\\s+Validation Loss:\\s+([\\d.]+)\")\n",
    "\n",
    "for log_file in log_files:\n",
    "    print(f\"Processing {log_file}\")\n",
    "    with open(log_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                epoch, total_epochs, train_loss, val_loss = match.groups()\n",
    "                print(f\"\\t Epoch {epoch}/{total_epochs} - Training Loss: {train_loss} - Validation Loss: {val_loss}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe13f0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 18:17:03.259699: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-02 18:17:03.261904: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 18:17:03.302229: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-02 18:17:03.303193: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-02 18:17:04.200669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strawberry Fields: a Python library for continuous-variable quantum circuits.\n",
      "Copyright 2018-2020 Xanadu Quantum Technologies Inc.\n",
      "\n",
      "Python version:            3.8.20\n",
      "Platform info:             Linux-4.18.0-477.27.1.el8_8.x86_64-x86_64-with-glibc2.17\n",
      "Installation path:         /rds/general/user/lr1424/home/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields\n",
      "Strawberry Fields version: 0.23.0\n",
      "Numpy version:             1.24.3\n",
      "Scipy version:             1.10.1\n",
      "SymPy version:             1.13.3\n",
      "NetworkX version:          3.1\n",
      "The Walrus version:        0.21.0\n",
      "Blackbird version:         0.5.0\n",
      "XCC version:               0.3.1\n",
      "TensorFlow version:        2.13.1\n",
      "Num PUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "TensorFlow is built with CUDA (GPU) support.\n"
     ]
    }
   ],
   "source": [
    "import strawberryfields as sf\n",
    "import tensorflow as tf\n",
    "sf.about()\n",
    "print(\"Num PUs Available: \", len(tf.config.list_physical_devices()))\n",
    "print(tf.config.list_physical_devices())\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"TensorFlow is built with CUDA (GPU) support.\")\n",
    "else:\n",
    "    print(\"TensorFlow is CPU-only (no GPU support).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5121cc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.0\n"
     ]
    }
   ],
   "source": [
    "import importlib.metadata\n",
    "print(importlib.metadata.version('typing_extensions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae92340",
   "metadata": {},
   "source": [
    "# Heavily Simplified Version - Testing for non-zero gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cc0cb6",
   "metadata": {},
   "source": [
    "### Working basic example w/ demo circuit from [here](https://strawberryfields.ai/photonics/demos/run_tutorial_machine_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "783e6973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0  loss=0.9801  gnorms=['4.0e-01', '2.2e-10']\n",
      "step    5  loss=0.0005  gnorms=['1.3e-02', '1.3e-11']\n",
      "step   10  loss=0.0013  gnorms=['2.7e-02', '0.0e+00']\n",
      "step   15  loss=0.8988  gnorms=['8.6e-01', '2.6e-10']\n",
      "step   20  loss=0.8630  gnorms=['9.9e-01', '2.8e-09']\n",
      "step   25  loss=0.0091  gnorms=['1.2e-01', '4.5e-13']\n",
      "step   30  loss=0.7789  gnorms=['1.2e+00', '1.9e-09']\n",
      "step   35  loss=0.0239  gnorms=['2.4e-01', '3.6e-10']\n",
      "step   40  loss=0.6565  gnorms=['1.4e+00', '4.7e-10']\n",
      "step   45  loss=0.0560  gnorms=['4.6e-01', '6.2e-11']\n"
     ]
    }
   ],
   "source": [
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, Sgate, BSgate, CXgate\n",
    "import tensorflow as tf, numpy as np, h5py, os, random\n",
    "\n",
    "# ----------  hyper-params ----------\n",
    "CUT_OFF     = 7                 # fock cutoff dim\n",
    "WIRES       = 1\n",
    "LAYERS      = 1\n",
    "STEPS       = 50\n",
    "LR          = 0.01\n",
    "MAX_JETS    = 200               # keep it tiny for the demo\n",
    "DATA_DIR    = \"/rds/general/user/lr1424/home/1P1Qm_SF/flat_train/TTBar+ZJets_flat.h5\"\n",
    "# -----------------------------------\n",
    "\n",
    "# ----------  load a small dataset ----------\n",
    "with h5py.File(DATA_DIR, \"r\") as f:\n",
    "    X = f[\"jetConstituentsList\"][:MAX_JETS, :WIRES, :]     # (N,4,3)\n",
    "    y = f[\"truth_labels\"][:MAX_JETS].astype(np.float32)     # (N,)\n",
    "\n",
    "jets   = tf.convert_to_tensor(X, tf.float32)\n",
    "labels = tf.convert_to_tensor(y, tf.float32)\n",
    "\n",
    "prog = sf.Program(WIRES)\n",
    "\n",
    "alpha, phi = prog.params(\"alpha\", \"phi\")\n",
    "\n",
    "with prog.context as q:\n",
    "    Dgate(alpha, phi) | q[0]\n",
    "\n",
    "tf_alpha = tf.Variable(0.1)\n",
    "tf_phi = tf.Variable(0.1)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(LR)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "eng = sf.Engine(\"tf\", backend_options={\"cutoff_dim\": CUT_OFF})\n",
    "\n",
    "# -------- training loop ----------\n",
    "for step in range(STEPS):\n",
    "    i = random.randrange(MAX_JETS)\n",
    "    jet, label = jets[i], labels[i]\n",
    "\n",
    "    if eng.run_progs:\n",
    "        eng.reset()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        state = eng.run(prog, args={\"alpha\": tf_alpha, \"phi\": tf_phi}).state\n",
    "        # print(state)\n",
    "        # print(state.mean_photon(0))\n",
    "        # print(tf.shape(state.mean_photon(0)),  \"---\", tf.shape(label))\n",
    "        mean_photon = state.mean_photon(0)[0]          # discard the variance\n",
    "        y_pred      = tf.expand_dims(mean_photon, 0)   # shape (1,)\n",
    "        y_true      = tf.expand_dims(label, 0)         # shape (1,)\n",
    "\n",
    "        loss = mse(y_true, y_pred)\n",
    "\n",
    "    vars_ = [tf_alpha, tf_phi] \n",
    "    grads = tape.gradient(loss, vars_)\n",
    "    opt.apply_gradients(zip(grads, vars_))\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        gnorms = [tf.norm(g).numpy() if g is not None else 0.0 for g in grads]\n",
    "        print(f\"step {step:4d}  loss={loss.numpy():.4f}  \"\n",
    "              f\"gnorms={['%.1e'%n for n in gnorms[:6]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a1534",
   "metadata": {},
   "source": [
    "### Working with simplified version of my circuit (after scaling pT down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31dddb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0  loss=0.6722  gnorms=['2.1e-02']\n",
      "step    5  loss=0.6993  gnorms=['4.6e-03']\n",
      "step   10  loss=0.6862  gnorms=['2.4e-03']\n",
      "step   15  loss=0.6442  gnorms=['3.7e-02']\n",
      "step   20  loss=0.6141  gnorms=['1.2e-02']\n",
      "step   25  loss=0.8035  gnorms=['1.1e-01']\n",
      "step   30  loss=0.7192  gnorms=['2.6e-02']\n",
      "step   35  loss=0.8194  gnorms=['1.3e-01']\n",
      "step   40  loss=0.6726  gnorms=['5.5e-03']\n",
      "step   45  loss=0.7056  gnorms=['8.1e-03']\n"
     ]
    }
   ],
   "source": [
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, Sgate, BSgate, CXgate\n",
    "import tensorflow as tf, numpy as np, h5py, os, random\n",
    "\n",
    "# ----------  hyper-params ----------\n",
    "CUT_OFF     = 7                 # fock cutoff dim\n",
    "WIRES       = 1\n",
    "LAYERS      = 1\n",
    "STEPS       = 50\n",
    "LR          = 0.01\n",
    "MAX_JETS    = 200               # keep it tiny for the demo\n",
    "DATA_DIR    = \"/rds/general/user/lr1424/home/1P1Qm_SF/flat_train/TTBar+ZJets_flat.h5\"\n",
    "# -----------------------------------\n",
    "\n",
    "# ----------  load a small dataset ----------\n",
    "with h5py.File(DATA_DIR, \"r\") as f:\n",
    "    X = f[\"jetConstituentsList\"][:MAX_JETS, :WIRES, :]     # (N,4,3)\n",
    "    y = f[\"truth_labels\"][:MAX_JETS].astype(np.float32)     # (N,)\n",
    "\n",
    "jets   = tf.convert_to_tensor(X, tf.float32)\n",
    "labels = tf.convert_to_tensor(y, tf.float32)\n",
    "\n",
    "# -------- symbolic circuit ----------\n",
    "prog = sf.Program(WIRES)\n",
    "s_scale = prog.params(\"s_scale\")\n",
    "\n",
    "eta = prog.params(\"eta\")\n",
    "phi = prog.params(\"phi\")\n",
    "pt  = prog.params(\"pt\")\n",
    "\n",
    "with prog.context as q:\n",
    "    scale = 10.0 / (1 + sf.math.exp(-s_scale)) + 0.01\n",
    "    Sgate(eta, pt*phi/2) | q[0]\n",
    "    Dgate(scale*pt, eta)    | q[0]\n",
    "    # CXgate(1.0) | (q[0], q[1])\n",
    "\n",
    "# # -------- one tf.Variable per placeholder  ############## FIX ##############\n",
    "rnd = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "tf_s_scale = tf.Variable(rnd(()))\n",
    "\n",
    "def make_args(jet):\n",
    "    d = {\"s_scale\": tf_s_scale}    \n",
    "    pt, eta, phi = jet[0, 2], jet[0, 0], jet[0, 1]\n",
    "    # Scale pt to [0, 1] via Aritra's case_reader.py\n",
    "    # Use dictionaries for feature limits\n",
    "    assumed_limits = {\n",
    "        'pt': [1e-4, 3000.0],    # assumed limits for pt\n",
    "        'eta': [-0.8, 0.8],      # example limits for eta\n",
    "        'phi': [-0.8, 0.8],  # example limits for phi\n",
    "    }\n",
    "    feature_limits = {\n",
    "        'pt': [0.0, 1.0],        # limits for the feature space\n",
    "        'eta': [-np.pi, np.pi],\n",
    "        'phi': [-np.pi, np.pi],\n",
    "    }\n",
    "    scaled_pt = ((pt - assumed_limits[\"pt\"][0]) / (assumed_limits[\"pt\"][1] - assumed_limits[\"pt\"][0])) * (feature_limits[\"pt\"][1] - feature_limits[\"pt\"][0]) + feature_limits[\"pt\"][0]\n",
    "    scaled_eta = ((eta - assumed_limits[\"eta\"][0]) / (assumed_limits[\"eta\"][1] - assumed_limits[\"eta\"][0])) * (feature_limits[\"eta\"][1] - feature_limits[\"eta\"][0]) + feature_limits[\"eta\"][0]\n",
    "    scaled_phi = ((phi - assumed_limits[\"phi\"][0]) / (assumed_limits[\"phi\"][1] - assumed_limits[\"phi\"][0])) * (feature_limits[\"phi\"][1] - feature_limits[\"phi\"][0]) + feature_limits[\"phi\"][0]\n",
    "    # print(\"pt =\", pt, \"scaled_pt =\",scaled_pt)\n",
    "    pt = scaled_pt\n",
    "    eta = scaled_eta\n",
    "    phi = scaled_phi\n",
    "\n",
    "    d[f\"eta\"] = eta\n",
    "    d[f\"phi\"] = phi\n",
    "    d[f\"pt\"]  = pt\n",
    "\n",
    "    return d\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(LR)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "eng = sf.Engine(\"tf\", backend_options={\"cutoff_dim\": CUT_OFF})\n",
    "\n",
    "# -------- training loop ----------\n",
    "for step in range(STEPS):\n",
    "    i = random.randrange(MAX_JETS)\n",
    "    jet, label = jets[i], labels[i]\n",
    "\n",
    "    if eng.run_progs:\n",
    "        eng.reset()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        state = eng.run(prog, args=make_args(jet)).state\n",
    "        mean_photon = state.mean_photon(0)[0]          # discard the variance\n",
    "        y_pred      = tf.expand_dims(mean_photon, 0)   # shape (1,)\n",
    "        y_true      = tf.expand_dims(label, 0)         # shape (1,)\n",
    "\n",
    "        loss = bce(y_true, y_pred)\n",
    "\n",
    "    vars_ = [tf_s_scale]\n",
    "    grads = tape.gradient(loss, vars_)\n",
    "    opt.apply_gradients(zip(grads, vars_))\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        gnorms = [tf.norm(g).numpy() if g is not None else 0.0 for g in grads]\n",
    "        print(f\"step {step:4d}  loss={loss.numpy():.4f}  \"\n",
    "              f\"gnorms={['%.1e'%n for n in gnorms[:6]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50b592e",
   "metadata": {},
   "source": [
    "### Seemingly working with full circuit ... except"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52872a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0  loss=0.0568  gnorms=['2.3e-03', '2.6e-03', '8.9e-03', '6.7e-03', '1.0e-03', '8.2e-05']\n",
      "step    5  loss=2.0657  gnorms=['4.4e-01', '3.3e-01', '4.5e-01', '3.8e-01', '1.1e-02', '2.2e-03']\n",
      "step   10  loss=1.7634  gnorms=['3.0e-01', '7.6e-02', '4.1e-01', '5.1e-01', '1.3e-02', '2.4e-05']\n",
      "step   15  loss=1.7323  gnorms=['4.1e-01', '3.5e-02', '3.8e-01', '5.4e-01', '2.1e-02', '1.2e-03']\n",
      "step   20  loss=2.1158  gnorms=['3.6e-01', '4.4e-03', '5.6e-01', '2.3e-01', '3.1e-02', '6.5e-03']\n",
      "step   25  loss=0.0654  gnorms=['4.6e-03', '1.9e-03', '4.1e-03', '1.5e-02', '3.3e-03', '4.5e-04']\n",
      "step   30  loss=1.6088  gnorms=['2.1e-01', '1.2e-01', '2.3e-01', '2.3e-01', '3.1e-02', '3.5e-04']\n",
      "step   35  loss=1.5967  gnorms=['2.9e-01', '1.6e-01', '4.1e-02', '9.6e-02', '3.4e-02', '1.0e-02']\n",
      "step   40  loss=0.2684  gnorms=['4.7e-02', '2.5e-02', '3.8e-02', '5.8e-02', '1.2e-02', '4.7e-03']\n",
      "step   45  loss=1.3017  gnorms=['2.6e-01', '7.4e-02', '8.4e-02', '1.6e-01', '2.6e-02', '9.4e-04']\n"
     ]
    }
   ],
   "source": [
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, Sgate, BSgate, CXgate\n",
    "import tensorflow as tf, numpy as np, h5py, os, random\n",
    "\n",
    "# ----------  hyper-params ----------\n",
    "CUT_OFF     = 7                 # fock cutoff dim\n",
    "WIRES       = 4\n",
    "LAYERS      = 1\n",
    "STEPS       = 50\n",
    "LR          = 0.01\n",
    "MAX_JETS    = 200               # keep it tiny for the demo\n",
    "DATA_DIR    = \"/rds/general/user/lr1424/home/1P1Qm_SF/flat_train/TTBar+ZJets_flat.h5\"\n",
    "# -----------------------------------\n",
    "\n",
    "# ----------  load a small dataset ----------\n",
    "with h5py.File(DATA_DIR, \"r\") as f:\n",
    "    X = f[\"jetConstituentsList\"][:MAX_JETS, :WIRES, :]     # (N,4,3)\n",
    "    y = f[\"truth_labels\"][:MAX_JETS].astype(np.float32)     # (N,)\n",
    "\n",
    "jets   = tf.convert_to_tensor(X, tf.float32)\n",
    "labels = tf.convert_to_tensor(y, tf.float32)\n",
    "\n",
    "# -------- symbolic circuit ----------\n",
    "prog = sf.Program(WIRES)\n",
    "s_scale = prog.params(\"s_scale\")\n",
    "DM  = [prog.params(f\"DM{w}\") for w in range(WIRES)]\n",
    "DP  = [prog.params(f\"DP{w}\") for w in range(WIRES)]\n",
    "SM  = [prog.params(f\"SM{w}\") for w in range(WIRES)]\n",
    "SP  = [prog.params(f\"SP{w}\") for w in range(WIRES)]\n",
    "eta = [prog.params(f\"eta{w}\") for w in range(WIRES)]\n",
    "phi = [prog.params(f\"phi{w}\") for w in range(WIRES)]\n",
    "pt  = [prog.params(f\"pt{w}\")  for w in range(WIRES)]\n",
    "\n",
    "\n",
    "with prog.context as q:\n",
    "    scale = 10.0 / (1.0 + sf.math.exp(-s_scale)) + 0.01\n",
    "    for w in range(WIRES):\n",
    "        Sgate(eta[w], pt[w]*phi[w]/2) | q[w]\n",
    "        Dgate(scale*pt[w], eta[w])    | q[w]\n",
    "    for a,b in [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)]:\n",
    "        CXgate(1.0) | (q[a], q[b])\n",
    "    for w in range(WIRES):\n",
    "        Sgate(SM[w], SP[w]) | q[w]\n",
    "        Dgate(DM[w], DP[w]) | q[w]\n",
    "\n",
    "# # -------- one tf.Variable per placeholder \n",
    "rnd = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "tf_s_scale = tf.Variable(rnd(()))\n",
    "tf_DM = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_DP = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_SM = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_SP = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "\n",
    "assumed_limits = {\n",
    "    'pt':  [1e-4, 3000.0],\n",
    "    'eta': [-0.8, 0.8],\n",
    "    'phi': [-0.8, 0.8],\n",
    "}\n",
    "feature_limits = {\n",
    "    'pt':  [0.0, 1.0],\n",
    "    'eta': [-np.pi, np.pi],\n",
    "    'phi': [-np.pi, np.pi],\n",
    "}\n",
    "\n",
    "def scale_feature(value, name):\n",
    "    \"\"\"Affine-map any feature from its assumed data range to the network range.\"\"\"\n",
    "    a_min, a_max = assumed_limits[name]\n",
    "    f_min, f_max = feature_limits[name]\n",
    "    return (value - a_min) / (a_max - a_min) * (f_max - f_min) + f_min\n",
    "\n",
    "def make_args(jet):\n",
    "    d = {\"s_scale\": tf_s_scale}\n",
    "    for w in range(WIRES):\n",
    "        d[f\"DM{w}\"] = tf_DM[w]\n",
    "        d[f\"DP{w}\"] = tf_DP[w]\n",
    "        d[f\"SM{w}\"] = tf_SM[w]\n",
    "        d[f\"SP{w}\"] = tf_SP[w]\n",
    "\n",
    "        d[f\"eta{w}\"] = scale_feature(jet[w, 0], \"eta\")\n",
    "        d[f\"phi{w}\"] = scale_feature(jet[w, 1], \"phi\")\n",
    "        d[f\"pt{w}\"]  = scale_feature(jet[w, 2], \"pt\")\n",
    "    return d\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(LR)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "eng = sf.Engine(\"tf\", backend_options={\"cutoff_dim\": CUT_OFF})\n",
    "\n",
    "# -------- training loop ----------\n",
    "for step in range(STEPS):\n",
    "    i = random.randrange(MAX_JETS)\n",
    "    jet, label = jets[i], labels[i]\n",
    "    # print(\"pt0 =\", jet[0, 2].numpy())      # add just after you sample `jet`\n",
    "    \n",
    "    # print(\"scale*pt =\", tf_s_scale*pt)\n",
    "    # print(\"pt*pt*phi/2 =\", pt*pt*phi/2)\n",
    "\n",
    "\n",
    "    if eng.run_progs:\n",
    "        eng.reset()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        state = eng.run(prog, args=make_args(jet)).state\n",
    "        photons = tf.stack([state.mean_photon(m) for m in range(3)])\n",
    "        logit   = tf.reduce_sum(photons)\n",
    "        loss    = bce(tf.expand_dims(label,0), tf.expand_dims(logit,0))\n",
    "        # mean_photon = state.mean_photon(0)[0]          # discard the variance\n",
    "        # y_pred      = tf.expand_dims(mean_photon, 0)   # shape (1,)\n",
    "        # y_true      = tf.expand_dims(label, 0)         # shape (1,)\n",
    "\n",
    "        # loss = bce(y_true, y_pred)\n",
    "\n",
    "    vars_ = [tf_s_scale, *tf_DM, *tf_DP, *tf_SM, *tf_SP]  \n",
    "    grads = tape.gradient(loss, vars_)\n",
    "    opt.apply_gradients(zip(grads, vars_))\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        gnorms = [tf.norm(g).numpy() if g is not None else 0.0 for g in grads]\n",
    "        print(f\"step {step:4d}  loss={loss.numpy():.4f}  \"\n",
    "              f\"gnorms={['%.1e'%n for n in gnorms[:6]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf769dc2",
   "metadata": {},
   "source": [
    "### Same as above (with additional debugging); keeping above to show it worked at least once.\n",
    "\n",
    "- Only works first time, then get the error:\n",
    "```\n",
    "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss.\n",
    "step    0  loss=0.2500  gnorms=['0.0e+00', '8.2e-03', '1.2e-03', '1.7e-02', '2.9e-03', '2.5e-05']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84b931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  grad None; pT’s were [240.02118  125.55173   78.10409   55.470455]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step    0  loss=2.3560  gnorms=['0.0e+00', '3.2e-01', '3.2e-01', '5.5e-01', '1.6e-02', '4.1e-03']\n",
      "⚠️  grad None; pT’s were [713.04114   23.53077   23.516682  18.784554]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [496.59155  137.78369  114.573     50.280598]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [68.94876  50.186806 49.476826 49.182854]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [95.76376  59.773476 38.720997 38.42305 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [80.27804  54.4434   44.192066 29.433529]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step    5  loss=2.2157  gnorms=['0.0e+00', '1.1e-01', '3.3e-03', '2.4e-02', '7.9e-02', '2.0e-03']\n",
      "⚠️  grad None; pT’s were [307.49683  145.66698   95.09012   37.446888]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [88.96303  77.79551  71.32266  37.119316]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [176.4177   130.43      79.05531   50.509792]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [192.16554   76.00975   68.331215  55.938663]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [135.75758   88.16328   74.204605  67.8978  ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   10  loss=1.7244  gnorms=['0.0e+00', '8.2e-02', '4.9e-03', '6.2e-02', '1.1e-01', '3.0e-04']\n",
      "⚠️  grad None; pT’s were [131.32982   46.217327  44.82381   40.14291 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [167.52127  109.466484  33.820866  33.645344]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [183.685    129.65437   71.41582   55.200485]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [85.50569  56.90149  55.927654 44.716248]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [147.25371   94.497826  72.313354  63.124825]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   15  loss=1.6713  gnorms=['0.0e+00', '5.4e-02', '3.7e-03', '6.9e-02', '1.8e-01', '1.3e-03']\n",
      "⚠️  grad None; pT’s were [105.29581   80.17091   55.43362   52.169064]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [528.91797   76.095604  71.475266  67.285164]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [119.121124  88.28405   77.79406   25.640575]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [311.3022    80.87379   64.353325  37.283276]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [95.76376  59.773476 38.720997 38.42305 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   20  loss=0.2217  gnorms=['0.0e+00', '1.4e-02', '7.2e-02', '6.6e-02', '4.1e-02', '1.0e-03']\n",
      "⚠️  grad None; pT’s were [103.07936   65.66356   59.921337  35.50682 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [496.59155  137.78369  114.573     50.280598]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [170.01709  126.683624  84.12245   37.829567]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [78.78082  66.9963   60.487396 57.776623]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [82.48329  59.546528 31.297403 30.975212]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   25  loss=0.2062  gnorms=['0.0e+00', '4.0e-02', '1.9e-02', '6.5e-02', '4.6e-02', '2.3e-03']\n",
      "⚠️  grad None; pT’s were [153.2043   115.06419   85.91323   52.596565]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [101.980125  83.09397   67.080536  65.55109 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [249.16507 229.4772  141.47795 119.42879]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [90.83638  78.45432  55.937218 32.83849 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [142.98346   52.594772  46.87868   37.33614 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   30  loss=0.1114  gnorms=['0.0e+00', '8.8e-04', '1.5e-03', '9.0e-03', '7.4e-02', '4.2e-04']\n",
      "⚠️  grad None; pT’s were [131.32982   46.217327  44.82381   40.14291 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [355.6535   100.81319   46.165573  37.42865 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [183.685    129.65437   71.41582   55.200485]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [234.48398 183.74269 166.40121  77.49663]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [143.00447  91.31098  52.10622  45.97724]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   35  loss=1.9114  gnorms=['0.0e+00', '4.2e-02', '1.4e-02', '6.8e-02', '5.5e-01', '2.5e-02']\n",
      "⚠️  grad None; pT’s were [181.27162  124.893974  92.017654  79.68922 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [222.02205  163.74416   79.30016   72.800156]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [77.07051  71.790565 58.87971  47.367134]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [142.98346   52.594772  46.87868   37.33614 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [135.81636 135.55241  64.91834  54.54412]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   40  loss=1.4131  gnorms=['0.0e+00', '1.8e-01', '3.6e-02', '1.9e-01', '3.6e-01', '9.5e-03']\n",
      "⚠️  grad None; pT’s were [144.90071 118.97869  78.96613  71.78979]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [234.7946   101.910706  88.41417   77.77244 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [243.61682 188.50275 184.53926 145.30283]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [104.214905  73.28705   57.4532    43.88384 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [176.4177   130.43      79.05531   50.509792]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   45  loss=1.7244  gnorms=['0.0e+00', '4.4e-02', '2.9e-01', '6.7e-01', '7.4e-01', '1.0e-02']\n",
      "⚠️  grad None; pT’s were [95.86513  45.439594 39.461376 37.90011 ]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [136.06564   62.128517  58.50667   54.883976]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [80.27804  54.4434   44.192066 29.433529]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⚠️  grad None; pT’s were [113.690506  94.9742    71.40072   52.807995]\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, Sgate, BSgate, CXgate\n",
    "import tensorflow as tf, numpy as np, h5py, os, random\n",
    "\n",
    "# ----------  hyper-params ----------\n",
    "CUT_OFF     = 7                 # fock cutoff dim\n",
    "WIRES       = 4\n",
    "LAYERS      = 1\n",
    "STEPS       = 50\n",
    "LR          = 0.01\n",
    "MAX_JETS    = 200               # keep it tiny for the demo\n",
    "DATA_DIR    = \"/rds/general/user/lr1424/home/1P1Qm_SF/flat_train/TTBar+ZJets_flat.h5\"\n",
    "# -----------------------------------\n",
    "\n",
    "# ----------  load a small dataset ----------\n",
    "with h5py.File(DATA_DIR, \"r\") as f:\n",
    "    X = f[\"jetConstituentsList\"][:MAX_JETS, :WIRES, :]     # (N,4,3)\n",
    "    y = f[\"truth_labels\"][:MAX_JETS].astype(np.float32)     # (N,)\n",
    "\n",
    "jets   = tf.convert_to_tensor(X, tf.float32)\n",
    "labels = tf.convert_to_tensor(y, tf.float32)\n",
    "\n",
    "# -------- symbolic circuit ----------\n",
    "prog = sf.Program(WIRES)\n",
    "s_scale = prog.params(\"s_scale\")\n",
    "DM  = [prog.params(f\"DM{w}\") for w in range(WIRES)]\n",
    "DP  = [prog.params(f\"DP{w}\") for w in range(WIRES)]\n",
    "SM  = [prog.params(f\"SM{w}\") for w in range(WIRES)]\n",
    "SP  = [prog.params(f\"SP{w}\") for w in range(WIRES)]\n",
    "eta = [prog.params(f\"eta{w}\") for w in range(WIRES)]\n",
    "phi = [prog.params(f\"phi{w}\") for w in range(WIRES)]\n",
    "pt  = [prog.params(f\"pt{w}\")  for w in range(WIRES)]\n",
    "\n",
    "\n",
    "with prog.context as q:\n",
    "    scale = 10.0 / (1.0 + sf.math.exp(-s_scale)) + 0.01\n",
    "    for w in range(WIRES):\n",
    "        Sgate(eta[w], pt[w]*phi[w]/2) | q[w]\n",
    "        Dgate(scale*pt[w], eta[w])    | q[w]\n",
    "    for a,b in [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)]:\n",
    "        CXgate(1.0) | (q[a], q[b])\n",
    "    for w in range(WIRES):\n",
    "        Sgate(SM[w], SP[w]) | q[w]\n",
    "        Dgate(DM[w], DP[w]) | q[w]\n",
    "\n",
    "# # -------- one tf.Variable per placeholder \n",
    "rnd = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "tf_s_scale = tf.Variable(rnd(()))\n",
    "# print(\"tf_s_scale =\", tf_s_scale.numpy())\n",
    "tf_DM = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_DP = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_SM = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_SP = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "\n",
    "assumed_limits = {\n",
    "    'pt':  [1e-4, 3000.0],\n",
    "    'eta': [-0.8, 0.8],\n",
    "    'phi': [-0.8, 0.8],\n",
    "}\n",
    "feature_limits = {\n",
    "    'pt':  [0.0, 1.0],\n",
    "    'eta': [-np.pi, np.pi],\n",
    "    'phi': [-np.pi, np.pi],\n",
    "}\n",
    "\n",
    "# EPS = 1e-3          \n",
    "\n",
    "def scale_feature(value, name):\n",
    "    \"\"\"Affine-map any feature from its assumed data range to the network range.\"\"\"\n",
    "    a_min, a_max = assumed_limits[name]\n",
    "    f_min, f_max = feature_limits[name]\n",
    "    out = (value - a_min) / (a_max - a_min) * (f_max - f_min) + f_min\n",
    "    # if name == \"pt\":\n",
    "    #     out = tf.maximum(out, EPS)\n",
    "    return out\n",
    "\n",
    "def make_args(jet):\n",
    "    d = {\"s_scale\": tf_s_scale}\n",
    "    for w in range(WIRES):\n",
    "        d[f\"DM{w}\"] = tf_DM[w]\n",
    "        d[f\"DP{w}\"] = tf_DP[w]\n",
    "        d[f\"SM{w}\"] = tf_SM[w]\n",
    "        d[f\"SP{w}\"] = tf_SP[w]\n",
    "\n",
    "        d[f\"eta{w}\"] = scale_feature(jet[w, 0], \"eta\")\n",
    "        d[f\"phi{w}\"] = scale_feature(jet[w, 1], \"phi\")\n",
    "        d[f\"pt{w}\"]  = scale_feature(jet[w, 2], \"pt\")\n",
    "        # print(\"Dgate w/\", tf_s_scale.numpy()*d[f\"pt{w}\"].numpy())\n",
    "    return d\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(LR)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "eng = sf.Engine(\"tf\", backend_options={\"cutoff_dim\": CUT_OFF})\n",
    "\n",
    "# -------- training loop ----------\n",
    "for step in range(STEPS):\n",
    "    i = random.randrange(MAX_JETS)\n",
    "    jet, label = jets[i], labels[i]\n",
    "    if tf.reduce_all(jet[..., 2] == 0.0):\n",
    "        continue \n",
    "    # print(\"pt0 =\", jet[0, 2].numpy())      # add just after you sample `jet`\n",
    "    \n",
    "    # print(\"scale*pt =\", tf_s_scale*pt)\n",
    "    # print(\"pt*pt*phi/2 =\", pt*pt*phi/2)\n",
    "\n",
    "\n",
    "    if eng.run_progs:\n",
    "        eng.reset()\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(tf_s_scale)\n",
    "        state = eng.run(prog, args=make_args(jet)).state\n",
    "        photons = tf.stack([state.mean_photon(m) for m in range(3)])\n",
    "        logit   = tf.reduce_sum(photons)\n",
    "        loss    = bce(tf.expand_dims(label,0), tf.expand_dims(logit,0))\n",
    "        # mean_photon = state.mean_photon(0)[0]          # discard the variance\n",
    "        # y_pred      = tf.expand_dims(mean_photon, 0)   # shape (1,)\n",
    "        # y_true      = tf.expand_dims(label, 0)         # shape (1,)\n",
    "\n",
    "        # loss = bce(y_true, y_pred)\n",
    "\n",
    "    vars_ = [tf_s_scale, *tf_DM, *tf_DP, *tf_SM, *tf_SP]  \n",
    "    grads = tape.gradient(loss, vars_)\n",
    "    if grads[0] is None:\n",
    "        print(\"⚠️  grad None; pT’s were\", jet[:, 2].numpy())\n",
    "    opt.apply_gradients(zip(grads, vars_))\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        gnorms = [tf.norm(g).numpy() if g is not None else 0.0 for g in grads]\n",
    "        print(f\"step {step:4d}  loss={loss.numpy():.4f}  \"\n",
    "              f\"gnorms={['%.1e'%n for n in gnorms[:6]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59ba84",
   "metadata": {},
   "source": [
    "### Add test and Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a8aeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⟨n⟩ = 3.4734027\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step    0  loss=0.0305  gnorms=['0.0e+00', '1.5e-03', '2.1e-03', '3.2e-03', '1.3e-03', '1.6e-04']\n",
      "⟨n⟩ = 2.8781526\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.2450297\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 4.7176275\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 4.5668917\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 4.9504476\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step    5  loss=0.0071  gnorms=['0.0e+00', '2.8e-03', '1.2e-03', '1.4e-03', '3.0e-04', '3.3e-05']\n",
      "⟨n⟩ = 1.5067389\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.715891\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.6605167\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 4.8122396\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.9025989\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   10  loss=2.9560  gnorms=['0.0e+00', '2.0e-02', '3.3e-01', '3.1e-01', '8.3e-02', '2.0e-02']\n",
      "⟨n⟩ = 1.6540494\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 3.1371894\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 4.7245255\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 4.2510567\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 3.467289\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   15  loss=0.0307  gnorms=['0.0e+00', '3.4e-03', '6.6e-03', '8.4e-03', '5.7e-03', '8.7e-04']\n",
      "⟨n⟩ = 2.4578595\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.16289\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.7647659\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.0694866\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.2685585\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   20  loss=2.3670  gnorms=['0.0e+00', '1.2e-01', '1.1e-01', '4.3e-02', '2.2e-01', '2.0e-03']\n",
      "⟨n⟩ = 2.3735878\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.0818286\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.6800995\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 3.220101\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 3.0935752\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   25  loss=3.1379  gnorms=['0.0e+00', '2.0e-01', '9.7e-02', '7.9e-02', '4.5e-01', '1.0e-02']\n",
      "⟨n⟩ = 0.96816736\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.5099092\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.5510845\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 0.9293283\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.5844247\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   30  loss=0.0727  gnorms=['0.0e+00', '1.1e-02', '3.8e-03', '7.9e-03', '4.2e-02', '1.3e-03']\n",
      "⟨n⟩ = 1.2962216\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 3.073103\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 0.9738909\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.9761598\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 3.257539\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   35  loss=0.0378  gnorms=['0.0e+00', '4.3e-03', '9.7e-04', '1.9e-03', '1.5e-02', '2.1e-04']\n",
      "⟨n⟩ = 2.4630466\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.1690853\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.0298146\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.5326314\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.0484695\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   40  loss=2.1697  gnorms=['0.0e+00', '2.0e-02', '3.2e-01', '1.0e+00', '1.7e-01', '1.5e-02']\n",
      "⟨n⟩ = 1.0579814\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.3882554\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.3226843\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 2.4184043\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.8356093\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "step   45  loss=0.1480  gnorms=['0.0e+00', '8.4e-03', '9.9e-03', '6.3e-02', '5.5e-02', '2.0e-03']\n",
      "⟨n⟩ = 2.1269336\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.615596\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.6019021\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "⟨n⟩ = 1.1038017\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 120\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 114\u001b[0m, in \u001b[0;36mpredict_logits\u001b[0;34m(jets_tensor)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/engine.py:570\u001b[0m, in \u001b[0;36mLocalEngine.run\u001b[0;34m(self, program, args, compile_options, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mmeasurement_deps \u001b[38;5;129;01mand\u001b[39;00m eng_run_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshots\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    566\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeed-forwarding of measurements cannot be used together with multiple shots.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m             )\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogram_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meng_run_options\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/engine.py:306\u001b[0m, in \u001b[0;36mBaseEngine._run\u001b[0;34m(self, program, args, compile_options, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m p\u001b[38;5;241m.\u001b[39mbind_params(args)\n\u001b[1;32m    304\u001b[0m p\u001b[38;5;241m.\u001b[39mlock()\n\u001b[0;32m--> 306\u001b[0m _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_progs\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, TDMProgram) \u001b[38;5;129;01mand\u001b[39;00m received_rolled:\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/engine.py:430\u001b[0m, in \u001b[0;36mLocalEngine._run_program\u001b[0;34m(self, prog, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cmd \u001b[38;5;129;01min\u001b[39;00m prog\u001b[38;5;241m.\u001b[39mcircuit:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;66;03m# try to apply it to the backend and, if op is a measurement, store it in values\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cmd\u001b[38;5;241m.\u001b[39mreg):\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/ops.py:508\u001b[0m, in \u001b[0;36mGate.apply\u001b[0;34m(self, reg, backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m temp \u001b[38;5;241m=\u001b[39m [rr\u001b[38;5;241m.\u001b[39mind \u001b[38;5;28;01mfor\u001b[39;00m rr \u001b[38;5;129;01min\u001b[39;00m reg]\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# call the child class specialized _apply method\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m original_p0\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/ops.py:1683\u001b[0m, in \u001b[0;36mSgate._apply\u001b[0;34m(self, reg, backend, **kwargs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, reg, backend, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1682\u001b[0m     r, phi \u001b[38;5;241m=\u001b[39m par_evaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp)\n\u001b[0;32m-> 1683\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/backends/tfbackend/backend.py:194\u001b[0m, in \u001b[0;36mTFBackend.squeeze\u001b[0;34m(self, r, phi, mode)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSqueeze\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    193\u001b[0m     remapped_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remap_modes(mode)\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcircuit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremapped_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/backends/tfbackend/circuit.py:465\u001b[0m, in \u001b[0;36mCircuit.squeeze\u001b[0;34m(self, r, theta, mode)\u001b[0m\n\u001b[1;32m    463\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_batch(theta)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_incompatible_batches(r, theta)\n\u001b[0;32m--> 465\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueezer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cutoff_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_is_pure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state(new_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/backends/tfbackend/ops.py:1057\u001b[0m, in \u001b[0;36msqueezer\u001b[0;34m(r, theta, mode, in_modes, cutoff, pure, batched, dtype)\u001b[0m\n\u001b[1;32m   1055\u001b[0m theta \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(theta, dtype)\n\u001b[1;32m   1056\u001b[0m matrix \u001b[38;5;241m=\u001b[39m squeezer_matrix(r, theta, cutoff, batched, dtype)\n\u001b[0;32m-> 1057\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msingle_mode_gate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_modes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/strawberryfields/backends/tfbackend/ops.py:769\u001b[0m, in \u001b[0;36msingle_mode_gate\u001b[0;34m(matrix, mode, in_modes, pure, batched)\u001b[0m\n\u001b[1;32m    767\u001b[0m     transposed_axis \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    768\u001b[0m     einsum_inputs\u001b[38;5;241m.\u001b[39mappend(tf\u001b[38;5;241m.\u001b[39mtranspose(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mconj(matrix), transposed_axis))\n\u001b[0;32m--> 769\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meinsum_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/ops/special_math_ops.py:810\u001b[0m, in \u001b[0;36m_einsum_v1\u001b[0;34m(equation, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    807\u001b[0m   axes_to_sum \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    808\u001b[0m       \u001b[38;5;28mset\u001b[39m(temp_axis_labels) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m    809\u001b[0m       \u001b[38;5;28mset\u001b[39m(input_axis_labels[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(output_axis_labels))\n\u001b[0;32m--> 810\u001b[0m   temp, temp_axis_labels \u001b[38;5;241m=\u001b[39m \u001b[43m_einsum_v1_reduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_axis_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43minput_axis_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43maxes_to_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m missing_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(temp_axis_labels) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(output_axis_labels)\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_indices:\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/ops/special_math_ops.py:997\u001b[0m, in \u001b[0;36m_einsum_v1_reduction\u001b[0;34m(t0, t0_axis_labels, t1, t1_axis_labels, axes_to_sum)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, axes_str \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(axis_labels):\n\u001b[1;32m    996\u001b[0m   perm \u001b[38;5;241m=\u001b[39m [axes_str\u001b[38;5;241m.\u001b[39mfind(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m sorted_axes[i]]\n\u001b[0;32m--> 997\u001b[0m   inputs[i] \u001b[38;5;241m=\u001b[39m \u001b[43m_transpose_if_necessary\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m t0, t1 \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axes_to_sum:\n\u001b[1;32m   1001\u001b[0m   \u001b[38;5;66;03m# In the special case where there are no axes to sum over, reduce to mul()\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m   \u001b[38;5;66;03m# rather than to batch matrix multiplication.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/ops/special_math_ops.py:1052\u001b[0m, in \u001b[0;36m_transpose_if_necessary\u001b[0;34m(tensor, perm)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Like transpose(), but avoids creating a new tensor if possible.\"\"\"\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m perm \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(perm))):\n\u001b[0;32m-> 1052\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1054\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2193\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name, conjugate)\u001b[0m\n\u001b[1;32m   2190\u001b[0m   transpose_fn \u001b[38;5;241m=\u001b[39m gen_array_ops\u001b[38;5;241m.\u001b[39mtranspose\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m perm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtranspose_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2195\u001b[0m rank \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:11907\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m  11905\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m  11906\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m> 11907\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtranspose_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11908\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11909\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m  11910\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:11929\u001b[0m, in \u001b[0;36mtranspose_eager_fallback\u001b[0;34m(x, perm, name, ctx)\u001b[0m\n\u001b[1;32m  11927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranspose_eager_fallback\u001b[39m(x, perm, name, ctx):\n\u001b[1;32m  11928\u001b[0m   _attr_T, (x,) \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39margs_to_matching_eager([x], ctx, [])\n\u001b[0;32m> 11929\u001b[0m   _attr_Tperm, (perm,) \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11930\u001b[0m   _inputs_flat \u001b[38;5;241m=\u001b[39m [x, perm]\n\u001b[1;32m  11931\u001b[0m   _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTperm\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_Tperm)\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:251\u001b[0m, in \u001b[0;36margs_to_matching_eager\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[0;32m--> 251\u001b[0m   tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[1;32m    253\u001b[0m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[1;32m    254\u001b[0m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:324\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    323\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m--> 324\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:263\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implementation of constant.\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m ctx \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    272\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m trace\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/qml-env/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1012\u001b[0m, in \u001b[0;36mContext.executing_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread_local_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_eager\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import strawberryfields as sf\n",
    "from strawberryfields.ops import Dgate, Sgate, BSgate, CXgate\n",
    "import tensorflow as tf, numpy as np, h5py, os, random\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ----------  hyper-params ----------\n",
    "CUT_OFF     = 7                 # fock cutoff dim\n",
    "WIRES       = 4\n",
    "LAYERS      = 1\n",
    "STEPS       = 50\n",
    "LR          = 0.01\n",
    "MAX_JETS    = 200               # keep it tiny for the demo\n",
    "DATA_DIR    = \"/rds/general/user/lr1424/home/1P1Qm_SF/flat_train/TTBar+ZJets_flat.h5\"\n",
    "VAL_DIR     = \"/rds/general/user/lr1424/home/1P1Qm_SF/flat_val/TTBar+ZJets_flat.h5\"\n",
    "TEST_DIR    = \"/rds/general/user/lr1424/home/1P1Qm_SF/flat_test/TTBar+ZJets_flat.h5\"\n",
    "# -----------------------------------\n",
    "\n",
    "# ----------  load datasets ----------\n",
    "def load_data(path, max_jets=MAX_JETS):\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        X = f[\"jetConstituentsList\"][:max_jets, :WIRES, :]     # (N,4,3)\n",
    "        y = f[\"truth_labels\"][:max_jets].astype(np.float32)     # (N,)\n",
    "    return tf.convert_to_tensor(X, tf.float32), tf.convert_to_tensor(y, tf.float32)\n",
    "\n",
    "jets, labels = load_data(DATA_DIR)\n",
    "jets_val, labels_val = load_data(VAL_DIR)\n",
    "jets_test, labels_test = load_data(TEST_DIR)\n",
    "\n",
    "# -------- symbolic circuit ----------\n",
    "prog = sf.Program(WIRES)\n",
    "s_scale = prog.params(\"s_scale\")\n",
    "DM  = [prog.params(f\"DM{w}\") for w in range(WIRES)]\n",
    "DP  = [prog.params(f\"DP{w}\") for w in range(WIRES)]\n",
    "SM  = [prog.params(f\"SM{w}\") for w in range(WIRES)]\n",
    "SP  = [prog.params(f\"SP{w}\") for w in range(WIRES)]\n",
    "eta = [prog.params(f\"eta{w}\") for w in range(WIRES)]\n",
    "phi = [prog.params(f\"phi{w}\") for w in range(WIRES)]\n",
    "pt  = [prog.params(f\"pt{w}\")  for w in range(WIRES)]\n",
    "\n",
    "with prog.context as q:\n",
    "    scale = 10.0 / (1.0 + sf.math.exp(-s_scale)) + 0.01\n",
    "    for w in range(WIRES):\n",
    "        Sgate(eta[w], pt[w]*phi[w]/2) | q[w]\n",
    "        Dgate(scale*pt[w], eta[w])    | q[w]\n",
    "    for a,b in [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)]:\n",
    "        CXgate(1.0) | (q[a], q[b])\n",
    "    for w in range(WIRES):\n",
    "        Sgate(SM[w], SP[w]) | q[w]\n",
    "        Dgate(DM[w], DP[w]) | q[w]\n",
    "\n",
    "rnd = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "tf_s_scale = tf.Variable(rnd(()))\n",
    "tf_DM = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_DP = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_SM = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "tf_SP = [tf.Variable(rnd(())) for _ in range(WIRES)]\n",
    "\n",
    "assumed_limits = {\n",
    "    'pt':  [1e-4, 3000.0],\n",
    "    'eta': [-0.8, 0.8],\n",
    "    'phi': [-0.8, 0.8],\n",
    "}\n",
    "feature_limits = {\n",
    "    'pt':  [0.0, 1.0],\n",
    "    'eta': [-np.pi, np.pi],\n",
    "    'phi': [-np.pi, np.pi],\n",
    "}\n",
    "\n",
    "def scale_feature(value, name):\n",
    "    a_min, a_max = assumed_limits[name]\n",
    "    f_min, f_max = feature_limits[name]\n",
    "    return (value - a_min) / (a_max - a_min) * (f_max - f_min) + f_min\n",
    "\n",
    "def make_args(jet):\n",
    "    d = {\"s_scale\": tf_s_scale}\n",
    "    for w in range(WIRES):\n",
    "        d[f\"DM{w}\"] = tf_DM[w]\n",
    "        d[f\"DP{w}\"] = tf_DP[w]\n",
    "        d[f\"SM{w}\"] = tf_SM[w]\n",
    "        d[f\"SP{w}\"] = tf_SP[w]\n",
    "        d[f\"eta{w}\"] = scale_feature(jet[w, 0], \"eta\")\n",
    "        d[f\"phi{w}\"] = scale_feature(jet[w, 1], \"phi\")\n",
    "        d[f\"pt{w}\"]  = scale_feature(jet[w, 2], \"pt\")\n",
    "    return d\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(LR)\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "eng = sf.Engine(\"tf\", backend_options={\"cutoff_dim\": CUT_OFF})\n",
    "\n",
    "# -------- training loop ----------\n",
    "for step in range(STEPS):\n",
    "    i = random.randrange(MAX_JETS)\n",
    "    jet, label = jets[i], labels[i]\n",
    "    if eng.run_progs:\n",
    "        eng.reset()\n",
    "    with tf.GradientTape() as tape:\n",
    "        state = eng.run(prog, args=make_args(jet)).state\n",
    "        photons = tf.stack([state.mean_photon(m) for m in range(3)])\n",
    "        logit   = tf.reduce_sum(photons)\n",
    "        print(\"⟨n⟩ =\", tf.reduce_sum(photons).numpy())\n",
    "        loss    = bce(tf.expand_dims(label,0), tf.expand_dims(logit,0))\n",
    "    vars_ = [tf_s_scale, *tf_DM, *tf_DP, *tf_SM, *tf_SP]  \n",
    "    grads = tape.gradient(loss, vars_)\n",
    "    opt.apply_gradients(zip(grads, vars_))\n",
    "    if step % 5 == 0:\n",
    "        gnorms = [tf.norm(g).numpy() if g is not None else 0.0 for g in grads]\n",
    "        print(f\"step {step:4d}  loss={loss.numpy():.4f}  \"\n",
    "              f\"gnorms={['%.1e'%n for n in gnorms[:6]]}\")\n",
    "\n",
    "# --------- Evaluate and print AUC ---------\n",
    "def predict_logits(jets_tensor):\n",
    "    logits = []\n",
    "    for jet in jets_tensor:\n",
    "        state = eng.run(prog, args=make_args(jet)).state\n",
    "        photons = tf.stack([state.mean_photon(m) for m in range(3)])\n",
    "        logit = tf.reduce_sum(photons).numpy()\n",
    "        logits.append(logit)\n",
    "    return np.array(logits)\n",
    "\n",
    "logits_val = predict_logits(jets_val)\n",
    "auc_val = roc_auc_score(labels_val.numpy(), logits_val)\n",
    "print(f\"Validation AUC: {auc_val:.4f}\")\n",
    "\n",
    "logits_test = predict_logits(jets_test)\n",
    "auc_test = roc_auc_score(labels_test.numpy(), logits_test)\n",
    "print(f\"Test AUC: {auc_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
